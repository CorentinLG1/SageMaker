{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script mode on Amazon SageMaker\n",
    "\n",
    "\n",
    "Sript mode is a way to work on Machine learning on Amazon Sagemaker only providing the script for processing, training or inference. In this notebook we will focuse on the lowest level of the Script mode usage that is to say using the base class provided by Amzon SageMaker.\n",
    "\n",
    "This notebook will follow each parts of a usual ML workflow with some explaination of the different SageMaker command used.\n",
    "\n",
    "First we want to import the different packages and load the data to S3 if it is not already done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manage interactions with the Amazon SageMaker APIs and any other AWS services needed\n",
    "session = sagemaker.Session()\n",
    "#see the region in which we work\n",
    "region = session.boto_region_name\n",
    "print(\"AWS Region : {}\".format(region))\n",
    "#get the role of the running session\n",
    "role = sagemaker.get_execution_role()\n",
    "#get the bucket name of the session\n",
    "bucket = session.default_bucket()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now push the data to S3 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload the dataset to S3\n",
    "prefix = \"data_script_mode\"\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'data/dataset.csv')).upload_file('predictive_maintenance.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "This graph sum-up how SageMaker handles the processing task :\n",
    "\n",
    "<img src=\"images/smprocess.PNG\" width=\"600\" height=\"400\">\n",
    "\n",
    "A processing job requires the specification of a path to an input S3 bucket that holds the data to be processed. The job utilizes a provided script to perform the processing task. The resulting output data is then stored in a separate S3 path.\n",
    "\n",
    "S3 effectively manages the job environment by utilizing Docker containers. These containers can either be pre-built containers provided by SageMaker, which are accessible on the Elastic Container Registry (ECR), or custom containers created from custom images that must be pushed to ECR.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SageMaker provide different class to instantiate some processing object to run processing job : \n",
    "\n",
    "<img src=\"images/processing.PNG\" width=\"700\" height=\"500\">\n",
    "\n",
    "We will use the Processor class. To do so we first need to have a docker image in which we get the sript we want to run for processing. Let's build such an image and push it to ECR :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "ecr_repository = 'sagemaker-processing-container'\n",
    "tag = ':latest'\n",
    "processing_repository_uri = '{}.dkr.ecr.{}.amazonaws.com/{}'.format(account_id, region, ecr_repository + tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "./build_and_push.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One we have our image pushed on ECR, we need to implement a Processor object which will be used to launch the processing job, for more information about the processing class see https://sagemaker.readthedocs.io/en/stable/api/training/processing.html\n",
    "\n",
    "The ProcessingInput class represents an input source for a processing job in Amazon SageMaker. It encapsulates information about the input data location, such as the S3 bucket path, and any optional configurations or preprocessing steps required before the processing job begins.\n",
    "The ProcessingOutput class represents an output destination for a processing job. It contains information about where the processed data should be stored, including the S3 bucket path and any optional configurations or post-processing steps.\n",
    "We can add some argument which are passed with argparse to our processing script. See the processing.py file to have more information about the architecture of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "#first we instanciate the processor with the image uri of our ECR image, and as described above, we need to provide the entrypoint of the docker container\n",
    "processor = Processor(\n",
    "    role = role,\n",
    "    image_uri = \"222978838857.dkr.ecr.us-east-1.amazonaws.com/sagemaker-processing-container\",\n",
    "    instance_count = 1,\n",
    "    instance_type = \"local\",\n",
    "    entrypoint = [\"python3\", \"processing.py\"]\n",
    "    )\n",
    "#The path of our S3 bucket\n",
    "bucket_path = 's3://{}'.format(bucket)\n",
    "\n",
    "#we then launch the processing job\n",
    "processor.run(\n",
    "    inputs=[ProcessingInput(source=f\"{bucket_path}/{prefix}/data/dataset.csv\", destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test_data\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    arguments=[\"--train-test-split-ratio\", \"0.2\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One the job is completed, we can retrieve some information about it, espacially get the S3 path of the output data so that we can use it for the training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_job_description = processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description[\"ProcessingOutputConfig\"]\n",
    "\n",
    "for output in output_config[\"Outputs\"]:\n",
    "    if output[\"OutputName\"] == \"train_data\":\n",
    "        preprocessed_training_data = output[\"S3Output\"][\"S3Uri\"]\n",
    "    if output[\"OutputName\"] == \"test_data\":\n",
    "        preprocessed_test_data = output[\"S3Output\"][\"S3Uri\"]\n",
    "        \n",
    "#Observe the processed data \n",
    "training_features = pd.read_csv(preprocessed_training_data + \"/dataset_train.csv\", index_col = \"UDI\",nrows=10)\n",
    "print(\"Training features shape: {}\".format(training_features.shape))\n",
    "training_features.drop([\"Target\"], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training part is similar to the processing part in the code but has some difference on the way SageMaker handles the task.\n",
    "\n",
    "<img src=\"images/training.PNG\" width=\"500\" height=\"700\">\n",
    "\n",
    "(1) On the Jupyter Notebook, you need to instanciate the training object to make the API call to SageMaker, push the data to S3 if needed, and push the image to ECR if needed.\n",
    "\n",
    "\n",
    "(2) One you run the fit method of the estimator you instanciated, you call the SageMaker API with the create_training_job request (see : https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_training_job.html), SageMaker launch the EC2 instance with the information you provided in the request (which is basicaly a json file since we work with RESTful API)\n",
    "\n",
    "(3) The training job run on the EC2 instance, when it has finished, it stores the output on a S3 bucket (model artifact, logs...) and shutdown every instance.\n",
    "\n",
    "(4) All the training outputs are available on a S3 bucket and the model is ready to be deployed on an endpoint\n",
    "\n",
    "\n",
    "\n",
    "In our case, we load a SageMaker image from ECR and use it for training. Then we just have to provide the training script as the entrypoit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "training_image = image_uris.retrieve(framework='sklearn',region='us-east-1',version='1.2-1',image_scope='training')\n",
    "\n",
    "\n",
    "metric = {\n",
    "    'Name' : 'Accuracy', 'Regex' : 'Accuracy : ([0-9\\\\.]+)'\n",
    "}\n",
    "\n",
    "estimator = Estimator(\n",
    "    role = role,\n",
    "    instance_count = 1,\n",
    "    instance_type = \"local\",\n",
    "    base_job_name = \"job\",\n",
    "    image_uri = training_image,\n",
    "    entry_point = \"train.py\",\n",
    "    metric_definitions = [metric]\n",
    ")\n",
    "\n",
    "estimator.set_hyperparameters(\n",
    "    C = 1,\n",
    "    kernel = \"poly\"\n",
    ")\n",
    "\n",
    "\n",
    "estimator.fit({\"train\" : preprocessed_training_data, \"test\" : preprocessed_test_data})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the processing, we can retrieve some information about the job. For example, the S3 path of our model to use it for inference if we want to use a custom inference script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_description = estimator.jobs[-1].describe()\n",
    "training_job_description\n",
    "model_data_s3_uri = \"{}\".format(training_job_description[\"ModelArtifacts\"][\"S3ModelArtifacts\"])\n",
    "model_data_s3_uri"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "\n",
    "Once the training is done, our model is ready to be deployed to and enpoint. We could directly use the deploy() method on our estimator but here we have not implemented an inference part in our training script and we want to use a different image for training and inference.\n",
    "We will use the class Model to deploy our model to an enpoint :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "inference_image = image_uris.retrieve(framework='sklearn',region='us-east-1',version='1.2-1',image_scope='inference')\n",
    "\n",
    "model = Model(\n",
    "    image_uri = inference_image,\n",
    "    model_data = model_data_s3_uri,\n",
    "    role = role,\n",
    "    entry_point = \"inference.py\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictor = model.deploy(    \n",
    "    initial_instance_count = 1,\n",
    "    instance_type = \"local\",\n",
    "    endpoint_name = \"myendpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
